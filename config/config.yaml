# RefYOLO-Human Configuration
# Central configuration for dataset paths and runtime settings
# Works on: local VS Code (CPU), Lightning AI Studio (GPU)

dataset:
  # Directory containing training images
  images_dir: data/images
  # Path to COCO-format annotations JSON
  annotations_path: data/annotations.json

cache:
  # Directory for cached YOLO features
  features_dir: cache

models:
  # YOLO model paths (relative to project root)
  # These are the BASE models (pretrained) - used if yolo.fine_tune is false
  pose_model: yolo11n-pose.pt
  seg_model: yolo11n-seg.pt

# =============================================================================
# YOLO FINE-TUNING CONFIGURATION
# =============================================================================
# YOLO fine-tuning is COMPLETE. Fine-tuned weights are in YOLO_finetuning/
# DO NOT run fine-tuning again - use the existing weights below.
yolo:
  # Base model for fine-tuning (reference only - NOT used for inference)
  pose_model: yolo11l-pose.pt
  seg_model: yolo11l-seg.pt
  # Fine-tuning mode - enables use of fine-tuned weights for caching/inference
  fine_tune: true
  # Training hyperparameters (reference only - fine-tuning already complete)
  epochs: 30
  batch_size: 16
  img_size: 640
  learning_rate: 0.001
  weight_decay: 0.0005
  # Output directory for fine-tuned weights (legacy - keeping for compatibility)
  output_dir: YOLO_finetuning
  # ==========================================================================
  # FINE-TUNED WEIGHTS - THESE ARE ACTIVELY USED
  # ==========================================================================
  # Paths to fine-tuned weights (REQUIRED when fine_tune=true)
  # These files MUST exist before running feature caching or training
  pose_finetuned: YOLO_finetuning/pose_best.pt
  seg_finetuned: YOLO_finetuning/seg_best.pt

training:
  # Device: "cuda" for GPU, "cpu" for CPU-only
  device: cuda
  # Batch size for training
  batch_size: 16
  # Number of training epochs
  num_epochs: 50
  # DataLoader workers (0 for main thread)
  num_workers: 0
  # Learning rate
  learning_rate: 1.0e-4
  # Weight decay for AdamW
  weight_decay: 1.0e-4
  # Gradient clipping max norm
  grad_clip_norm: 1.0
  # Max steps per epoch (null for full dataset)
  max_steps_per_epoch: null

checkpoints:
  # Directory for saving checkpoints
  save_dir: checkpoints
  # Save checkpoint every N epochs
  save_every_epochs: 1
  # Keep best model based on margin success rate
  keep_best: true

runtime:
  # Random seed for reproducibility
  seed: 42
  # Enable deterministic mode
  deterministic: true
  # Enable mixed precision (AMP) - only on CUDA
  mixed_precision: false

# =============================================================================
# SAMPLE-LEVEL SPLITS
# =============================================================================
# Splitting is done at the SAMPLE (caption-instance) level, NOT image level.
# A sample = (image_id, caption, gt_human_index, candidate_humans)
# This is intentional for referring expression grounding.
splits:
  # Train/val/test ratios (MUST sum to exactly 1.0)
  train: 0.8
  val: 0.1
  test: 0.1
  # Seed for deterministic shuffling (separate from runtime seed)
  seed: 42

# =============================================================================
# GROUNDING ADAPTER CONFIGURATION
# =============================================================================
# Controls the fusion mechanism between text query and visual tokens
grounding:
  # Adapter type: "cross_attention" (Phase-1 improvement) or "film" (baseline)
  adapter_type: cross_attention
  # Cross-attention specific settings (ignored if adapter_type=film)
  cross_attention:
    # Number of attention heads (4 or 8)
    num_heads: 4
    # Number of transformer layers (1 or 2)
    num_layers: 1
    # FFN hidden dimension
    dim_feedforward: 512
    # Dropout rate (0.0 for deterministic)
    dropout: 0.1

# =============================================================================
# OUTPUTS CONFIGURATION
# =============================================================================
outputs:
  # Base output directory for all artifacts
  base_dir: outputs
  # Directory for CSV logs
  logs_dir: outputs/logs
  # Directory for visualizations
  visualizations_dir: outputs/visualizations
  # Directory for plots
  plots_dir: outputs/plots
  # Directory for evaluation results
  evaluation_dir: outputs/evaluation
