# RefYOLO-Human Configuration
# Central configuration for dataset paths and runtime settings
# Works on: local VS Code (CPU), Lightning AI Studio (GPU)

dataset:
  # Directory containing training images
  images_dir: data/images
  # Path to COCO-format annotations JSON
  annotations_path: data/annotations.json

cache:
  # Directory for cached YOLO features
  features_dir: cache

models:
  # YOLO model paths (relative to project root)
  # These are the BASE models (pretrained) - used if yolo.fine_tune is false
  pose_model: yolo11n-pose.pt
  seg_model: yolo11n-seg.pt

# =============================================================================
# YOLO FINE-TUNING CONFIGURATION
# =============================================================================
# YOLO fine-tuning is COMPLETE. Fine-tuned weights are in YOLO_finetuning/
# DO NOT run fine-tuning again - use the existing weights below.
yolo:
  # Base model for fine-tuning (reference only - NOT used for inference)
  pose_model: yolo11l-pose.pt
  seg_model: yolo11l-seg.pt
  # Fine-tuning mode - enables use of fine-tuned weights for caching/inference
  fine_tune: true
  # Training hyperparameters (reference only - fine-tuning already complete)
  epochs: 30
  batch_size: 16
  img_size: 640
  learning_rate: 0.001
  weight_decay: 0.0005
  # Output directory for fine-tuned weights (legacy - keeping for compatibility)
  output_dir: YOLO_finetuning
  # ==========================================================================
  # FINE-TUNED WEIGHTS - THESE ARE ACTIVELY USED
  # ==========================================================================
  # Paths to fine-tuned weights (REQUIRED when fine_tune=true)
  # These files MUST exist before running feature caching or training
  pose_finetuned: YOLO_finetuning/pose_best.pt
  seg_finetuned: YOLO_finetuning/seg_best.pt

training:
  # Device: "cuda" for GPU, "cpu" for CPU-only
  device: cuda
  # Batch size for training
  batch_size: 32
  # Number of training epochs
  num_epochs: 50
  # DataLoader workers (0 for main thread)
  num_workers: 0
  # Learning rate
  learning_rate: 1.0e-4
  # Weight decay for AdamW
  weight_decay: 1.0e-4
  # Gradient clipping max norm
  grad_clip_norm: 1.0
  # Max steps per epoch (null for full dataset)
  max_steps_per_epoch: null

checkpoints:
  # Directory for saving checkpoints
  save_dir: checkpoints
  # Save checkpoint every N epochs
  save_every_epochs: 1
  # Keep best model based on margin success rate
  keep_best: true

runtime:
  # Random seed for reproducibility
  seed: 42
  # Enable deterministic mode
  deterministic: true
  # Enable mixed precision (AMP) - only on CUDA
  mixed_precision: false

# =============================================================================
# SAMPLE-LEVEL SPLITS
# =============================================================================
# Splitting is done at the SAMPLE (caption-instance) level, NOT image level.
# A sample = (image_id, caption, gt_human_index, candidate_humans)
# This is intentional for referring expression grounding.
splits:
  # Train/val/test ratios (MUST sum to exactly 1.0)
  train: 0.8
  val: 0.1
  test: 0.1
  # Seed for deterministic shuffling (separate from runtime seed)
  seed: 42

# =============================================================================
# GROUNDING ADAPTER CONFIGURATION
# =============================================================================
# Controls the fusion mechanism between text query and visual tokens
grounding:
  # ==========================================================================
  # EXPERIMENT MODE (MASTER SWITCH)
  # ==========================================================================
  # Use this to quickly switch between experiment configurations.
  # When set, this OVERRIDES adapter_type and hard_negative_mining.enabled
  # Options:
  #   null     - Manual mode (use adapter_type and HNM settings directly)
  #   "phase1" - Cross-attention adapter only
  #   "phase2" - Cross-attention + hard negative mining
  #   "phase3" - Text-visual token alignment only
  #   "phase3_hnm" - Text-visual alignment + hard negative mining
  experiment_mode: phase3
  
  # Adapter type (used when experiment_mode is null):
  #   "film" - FiLM baseline (Phase-0)
  #   "cross_attention" - Cross-attention (Phase-1)
  #   "text_visual_alignment" - Token-level alignment (Phase-3)
  adapter_type: cross_attention
  
  # ==========================================================================
  # TEXT ENCODER SETTINGS
  # ==========================================================================
  # Controls text encoder model and tokenization
  text_encoder:
    # Model type: "minilm" (baseline, 384D) or "clip" (improved, 512D)
    # CLIP is trained for vision-language alignment and provides better embeddings
    model_type: clip
    # Maximum token sequence length (affects Phase-3 token-level grounding)
    # CLIP max is 77 tokens. Recommended: >= 32 to avoid truncating descriptions
    max_length: 64
  
  # Cross-attention specific settings (Phase-1)
  # Ignored if adapter_type != "cross_attention"
  cross_attention:
    # Number of attention heads (4 or 8)
    num_heads: 4
    # Number of transformer layers (1 or 2)
    num_layers: 1
    # FFN hidden dimension
    dim_feedforward: 512
    # Dropout rate (0.0 for deterministic)
    dropout: 0.1
  
  # ==========================================================================
  # TEXT-VISUAL ALIGNMENT (Phase-3)
  # ==========================================================================
  # Token-level cross-modal attention for fine-grained grounding
  # Replaces sentence-level embeddings with word-level alignment
  # Set adapter_type: text_visual_alignment to activate
  text_visual_alignment:
    # Number of attention heads
    num_heads: 4
    # Number of cross-attention layers
    num_layers: 1
    # FFN hidden dimension
    dim_feedforward: 512
    # Dropout rate
    dropout: 0.1
    # Enable bidirectional refinement (visual→text→visual)
    bidirectional: true
  
  # ==========================================================================
  # HARD NEGATIVE MINING (Phase-2)
  # ==========================================================================
  # Difficulty-aware negative sampling to improve discrimination
  # Set enabled: true to activate Phase-2 improvements
  hard_negative_mining:
    # Master switch - set to true for Phase-2 experiments
    enabled: false
    # Difficulty score weights (must sum to 1.0)
    # Higher weight = more influence on difficulty score
    weight_iou: 0.5      # IoU overlap with GT (spatial proximity)
    weight_pose: 0.3     # Keypoint similarity (pose similarity)
    weight_size: 0.2     # Bounding box size similarity
    # Curriculum scheduling - gradually increase hard negative focus
    curriculum_enabled: true
    curriculum_start_ratio: 0.3   # Start with 30% hard negatives
    curriculum_end_ratio: 0.9     # End with 90% hard negatives
    curriculum_warmup_epochs: 5   # Linear warmup over 5 epochs
    # Mining strategy
    top_k_hard: 4                 # Focus on top-4 hardest negatives
    hard_negative_weight: 2.0     # Extra loss weight for hard negatives

  # ==========================================================================
  # TRAINING AUGMENTATION
  # ==========================================================================
  # Feature and text augmentations to improve generalization
  augmentation:
    # Feature-level augmentation (applied to cached visual features)
    feature_dropout: 0.1         # Dropout rate for visual features (0.0-0.3)
    feature_noise_std: 0.02      # Gaussian noise std for features (0.0-0.05)
    # Caption paraphrasing (text-side augmentation)
    use_paraphrases: false       # Enable caption paraphrasing
    paraphrase_prob: 0.3         # Probability of using paraphrase

# =============================================================================
# OUTPUTS CONFIGURATION
# =============================================================================
outputs:
  # Base output directory for all artifacts
  base_dir: outputs
  # Directory for CSV logs
  logs_dir: outputs/logs
  # Directory for visualizations
  visualizations_dir: outputs/visualizations
  # Directory for plots
  plots_dir: outputs/plots
  # Directory for evaluation results
  evaluation_dir: outputs/evaluation
