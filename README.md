# RefYOLO Human

This repository implements a referring expression grounding system for human detection, combining YOLO-based visual perception with language model reasoning. The architecture follows a strict modular design: the **vision** module extracts bounding boxes, segmentation masks, keypoints, and visual embeddings from input images; the **adapter** module transforms these visual tokens and query embeddings into a unified representation suitable for language model consumption; the **llm** module processes the fused representations to produce grounding scores that rank detected humans against the input referring expression; and the **pipeline** module orchestrates the end-to-end flow with deterministic, CPU-compatible execution. All cross-module data exchange is governed by formal interface contracts defined in the `contracts/` directory, with strict type enforcement via immutable dataclasses in `core/datatypes.py` and runtime shape/value assertions in `core/assertions.py`.
